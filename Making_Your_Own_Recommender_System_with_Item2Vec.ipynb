{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will walk you through the ideas behind Word2Vec and one of its extensions - Item2Vec - in the non-Natrual Language Processing domain. Specifically, using the concept of Word2Vec and the module provided by the Gensim package, we are going to build a lightweight movie recommender system.\n",
    "\n",
    "Approaches like Word2Vec learn word embeddings that preserve the semantic characteristics of words and the relations between words. Item2Vec is one of their variants. It is an item-based Collaborative Filtering algorithm utilizing the embedding for the items in a latent space. The algorithm is proved to provided competitive results compared to the traditional SVD approach or even deep learning algorithms.\n",
    "\n",
    "Leveraging on the simple but powerful architecture of Word2vec, we can build a movie recommender system just with a standard multi-cored laptop! Isn't that exciting? Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial Content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will cover the following topics in this tutorial:\n",
    "- [A Brief Introduction of Word2Vec and Item2Vec](#A-Brief-Introduction-of-Word2Vec-and-Item2Vec)\n",
    "- [Preparing the Data](#Preparing-the-Data)\n",
    "- [Training the Model with Gensim](#Training-the-Model-with-Gensim)\n",
    "- [Let's Make Some Recommendations!](#Let's-Make-Some-Recommendations!)\n",
    "- [Evaluating the Model Performance](#Evaluating-the-Model-Performance)\n",
    "- [References](#References)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Brief Introduction of Word2Vec and Item2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of my tutorial is adapted from McCormick, C.'s. Word2Vec Tutorial. The adaption is for the completeness of this tutorial. I do no better job than McCormick on explaining the concept, so if you are interested in Word2Vec, definitely read his work [[1]](#mccormick)\n",
    "\n",
    "Word2vec is a collection of two shallow neural network architectures. Namely the “Continuous Bag of Words” (CBOW) and the “Skip-Gram” architecture. Those architectures describe how the neural network \"learns\" the weights for the nodes in each layer in the neural net. I will briefly explain the “Skip-Gram” architecture and that would be enough for us to understand Item2vec.\n",
    "\n",
    "For the Skip-Gram architecture, imagine the following task:\n",
    "Given a single word, predict the probability of the words in the vocabulary being nearby the given word. The notion of “nearby” is defined by the hyperparameter-“window size”. For example, the neighborhood defined by a window size of 2 is \"2 words proceed and 2 words precede the given word\". \n",
    "\n",
    "The Skip-Gram architecture uses the following layout to address the problem:\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1EW0iHMhnaqBELVM-gif5WoKdkf3j7Yoh\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "(The vocabulary size depends on the data, and the size of the middle layer is a hyperparameter)\n",
    "\n",
    "In this architecture, for a given word, its one-hot encoded word vector is projected into a lower dimension word embedding in the middle layer, and then transformed into a vector that specifies the probabilities of its surrounding words. For more, you can read on [this](http://mccormickml.com/assets/word2vec/Alex_Minnaar_Word2Vec_Tutorial_Part_I_The_Skip-Gram_Model.pdf).\n",
    "\n",
    "The machine learning optimization problem here is to learn the best projection matrix from the data. Wrod2Vec is a \"self-supervied\" algorithm, in the sense that, although we do not need to supply labeled data, a \"correct probability\" needs to be generated from the data for the algorithm to learn the weight. The process of generating the training examples is best illustrated by the following graph. The \"correct\" probability can be calculated according to the generated word pairs. Each training example would be passed into this structure to tweak the weight of the projection matrix. \n",
    "<img src=\"https://drive.google.com/uc?id=1fK150u7hi9Q577Dbu_b055K72fGNpctv\" alt=\"Drawing\" style=\"width: 800px;\"/>\n",
    "\n",
    "Excitingly, the word embedding learned in the middle layer in our architecture preserves the semantic properties and the relationships between words. Note that an arbitrary word embedding doesn’t necessarily have this property. For example, the word embedding created from one-hot-encoding in the input layer doesn’t have those properties.\n",
    "\n",
    "The complete architecture in the Word2Vec skip-gram mode includes negative sampling to reduce the computational complexity and improve the quality of the word vectors. The above describes the simple form of the skip-gram model, but that's all we need to understand item2vec. The item2vec architecture utilizes skip-gram and negative sampling, so just imagine each item is a “word”, each collection of items is a “sentence”, and our goal is to learn the item embeddings to figure out the relationships between items. For more on item2vec, you can read the original paper [[2]](##### [2] Oren Barkan and Noam Koenigstein. Item2vec: Neural item embedding for collaborative filtering. 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the MovieLens 20M Dataset curated by the MovieLens research team. It contains 20 million ratings and 465,000 tag applications applied to 27,000 movies by 138,000 users. For more details, you can visit the [official website](https://grouplens.org/datasets/movielens). You can download the dataset via this [link](http://files.grouplens.org/datasets/movielens/ml-20m.zip)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of building our recommender systems, we will use the \"movies.csv\" and \"ratings.csv\" file from the downloaded dataset. \"movies.csv\" is a lookup table for the movie's id and its name. \"ratings.csv\" contains the ratings of all the users for all the movies. The following code read the csv files, examine the data and visualize the distribution of the ratings. For convenience, I also create lookup dictionaries for movie ids and movie names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5015</th>\n",
       "      <td>5111</td>\n",
       "      <td>Good Son, The (1993)</td>\n",
       "      <td>Drama|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10003</th>\n",
       "      <td>32906</td>\n",
       "      <td>Ascent, The (Voskhozhdeniye) (1977)</td>\n",
       "      <td>Drama|War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16509</th>\n",
       "      <td>83381</td>\n",
       "      <td>Seven Thieves (1960)</td>\n",
       "      <td>Crime|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>846</th>\n",
       "      <td>861</td>\n",
       "      <td>Supercop (Police Story 3: Supercop) (Jing cha ...</td>\n",
       "      <td>Action|Comedy|Crime|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20155</th>\n",
       "      <td>99258</td>\n",
       "      <td>Who Wants to Kill Jessie? (Kdo chce zabít Jess...</td>\n",
       "      <td>Comedy|Sci-Fi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       movieId                                              title  \\\n",
       "5015      5111                               Good Son, The (1993)   \n",
       "10003    32906                Ascent, The (Voskhozhdeniye) (1977)   \n",
       "16509    83381                               Seven Thieves (1960)   \n",
       "846        861  Supercop (Police Story 3: Supercop) (Jing cha ...   \n",
       "20155    99258  Who Wants to Kill Jessie? (Kdo chce zabít Jess...   \n",
       "\n",
       "                             genres  \n",
       "5015                 Drama|Thriller  \n",
       "10003                     Drama|War  \n",
       "16509                   Crime|Drama  \n",
       "846    Action|Comedy|Crime|Thriller  \n",
       "20155                 Comedy|Sci-Fi  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of the total 27278 data points\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>movieId</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7937017</th>\n",
       "      <td>54688</td>\n",
       "      <td>47610</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1370061037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11079211</th>\n",
       "      <td>76564</td>\n",
       "      <td>1101</td>\n",
       "      <td>3.0</td>\n",
       "      <td>946438936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15516997</th>\n",
       "      <td>107317</td>\n",
       "      <td>1270</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1162693457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17771935</th>\n",
       "      <td>122871</td>\n",
       "      <td>5313</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1230660174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16371933</th>\n",
       "      <td>113342</td>\n",
       "      <td>780</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1356738841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          userId  movieId  rating   timestamp\n",
       "7937017    54688    47610     3.5  1370061037\n",
       "11079211   76564     1101     3.0   946438936\n",
       "15516997  107317     1270     0.5  1162693457\n",
       "17771935  122871     5313     3.5  1230660174\n",
       "16371933  113342      780     3.0  1356738841"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of the total 20000263 data points\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_movies = pd.read_csv('ml-20m/movies.csv')\n",
    "df_ratings = pd.read_csv('ml-20m/ratings.csv')\n",
    "\n",
    "movieId_to_name = pd.Series(df_movies.title.values, index = df_movies.movieId.values).to_dict()\n",
    "name_to_movieId = pd.Series(df_movies.movieId.values, index = df_movies.title).to_dict()\n",
    "\n",
    "# Randomly display 5 records in the dataframe\n",
    "for df in list((df_movies, df_ratings)):\n",
    "    rand_idx = np.random.choice(len(df), 5, replace=False)\n",
    "    display(df.iloc[rand_idx,:])\n",
    "    print(\"Displaying 5 of the total \"+str(len(df))+\" data points\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh0AAAGMCAYAAAB+shCcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMHRIkqQm/j9ONWCc24sO9QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x25c2e091438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "ax = plt.subplot(111)\n",
    "ax.set_title(\"Distribution of Movie Ratings\", fontsize=16)\n",
    "ax.spines[\"top\"].set_visible(False)  \n",
    "ax.spines[\"right\"].set_visible(False)  \n",
    "  \n",
    "plt.xticks(fontsize=12)  \n",
    "plt.yticks(fontsize=12)  \n",
    "  \n",
    "plt.xlabel(\"Movie Rating\", fontsize=14)  \n",
    "plt.ylabel(\"Count\", fontsize=14)  \n",
    "  \n",
    "plt.hist(df_ratings['rating'], color=\"#3F5D7D\")  \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a standard machine learning pipeline, we would want to first separate the data into training and test sets. The test set is for the evaluation of the model. There are multiple ways to evaluate a recommender system, which affects how we split the data. You can read more [here](\"https://medium.com/@m_n_malaeb/recall-and-precision-at-k-for-recommender-systems-618483226c54\"). I will use precision, recall and F-1 score at K to evaluate model performance (explained in the evaluate performance section), so I'll do a stratified split on the userId. For each user, I split the movies he/she has rated to training and test data with a 70:30 ratio. With Scikit-Learn, we can do this in one line of code: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_ratings_train, df_ratings_test= train_test_split(df_ratings,\n",
    "                                                    stratify=df_ratings['userId'],\n",
    "                                                    random_state = 15688,\n",
    "                                                    test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training data: 14000184\n",
      "Number of test data: 6000079\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training data: \"+str(len(df_ratings_train)))\n",
    "print(\"Number of test data: \"+str(len(df_ratings_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our model to learn the embedding, we need to get the \"word\" and \"sentence\" equivalents from the data. Here you can imagine that each \"movie\" is a \"word\", and movies that received similar ratings from a user are in the same \"sentence\".\n",
    "\n",
    "Specifically, \"sentences\" are generated with the below process:\n",
    "For each user, generate 2 lists, which respectively stores the movies \"Liked\" and \"Disliked\" by the user. The first list contains all the movies are rated 4 points or above. The second list contains the rest of the movies. Those lists are the inputs to train the Gensim Word2Vec model.\n",
    "\n",
    "The following code does what was said above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rating_splitter(df):\n",
    "    \n",
    "    df['liked'] = np.where(df['rating']>=4, 1, 0)\n",
    "    df['movieId'] = df['movieId'].astype('str')\n",
    "    gp_user_like = df.groupby(['liked', 'userId'])\n",
    "\n",
    "    return ([gp_user_like.get_group(gp)['movieId'].tolist() for gp in gp_user_like.groups])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "splitted_movies = rating_splitter(df_ratings_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will feed the training data into the Gensim Word2Vec module. I will explain how to tweak the window size. You can read the full documentation for other parameters on the [official website]( https://radimrehurek.com/gensim/models/word2vec.html). I have put some explanation in the comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the original Word2Vec, the window size affects the scope we searched for \"contexts\" to define the meaning of a given word. As the way it is defined, the window is of a fixed size. However, in our Item2Vec implementation, the \"meaning\" of a movie should be captured by all its neighbors in the same list. In other words, we should consider all the movies \"Liked\" by a user to define the \"meaning\" for each of those movies. This applies to all the movies \"Disliked\" by a user too. The window size then needs to be changed according to the size of each movie lists.\n",
    "\n",
    "To address this problem without modifying the underlying code of the Gensim model, we first specify a very very large window size, which is way larger than the length of any movie lists in our training example. We then shuffle our training data before feeding them into the model, because the order of movies is not meaningful when defining the \"meaning\" of a movie using its neighbors.\n",
    "\n",
    "The window parameter in the Gensim model is in fact random dynamic. We specify the maximum window size instead of the actual window size used. Although workaround above is not ideal, it does achieve acceptable performance. The best approach might be modifying the underlying code in Gensim directly, but that would be beyond the scope of this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing before training the mode is to make sure that Gensim is utilizing the C compiler, run the below code to verify this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "\n",
    "import gensim\n",
    "assert gensim.models.word2vec.FAST_VERSION > -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code shuffles the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "for movie_list in splitted_movies:\n",
    "    random.shuffle(movie_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The below code trains the models. Two models are trained using different settings. Training the models may take a while, you can download my models [here](https://drive.google.com/open?id=1FTBsbPtYCAcS_N53MEr3VUwSqGfeIPHe) to test them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time passed: 2:12:26.134283\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "model = Word2Vec(sentences = splitted_movies, # We will supply the pre-processed list of moive lists to this parameter\n",
    "                 iter = 5, # epoch\n",
    "                 min_count = 10, # a movie has to appear more than 10 times to be keeped\n",
    "                 size = 200, # size of the hidden layer\n",
    "                 workers = 4, # specify the number of threads to be used for training\n",
    "                 sg = 1, # Defines the training algorithm. We will use skip-gram so 1 is chosen.\n",
    "                 hs = 0, # Set to 0, as we are applying negative sampling.\n",
    "                 negative = 5, # If > 0, negative sampling will be used. We will use a value of 5.\n",
    "                 window = 9999999)\n",
    "\n",
    "print(\"Time passed: \" + str(datetime.datetime.now()-start))\n",
    "#Word2Vec.save('item2vec_20180327')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time passed: 5:32:50.270232\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "model_w2v_sg = Word2Vec(sentences = splitted_movies,\n",
    "                        iter = 10, # epoch\n",
    "                        min_count = 5, # a movie has to appear more than 5 times to be keeped\n",
    "                        size = 300, # size of the hidden layer\n",
    "                        workers = 4, # specify the number of threads to be used for training\n",
    "                        sg = 1,\n",
    "                        hs = 0,\n",
    "                        negative = 5,\n",
    "                        window = 9999999)\n",
    "\n",
    "print(\"Time passed: \" + str(datetime.datetime.now()-start))\n",
    "model_w2v_sg.save('item2vec_word2vecSg_20180328')\n",
    "del model_w2v_sg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, the model can be saved in your storage for future use. Note that Gensim saved all the information about the model, including the hidden weights, vocabulary frequency and the binary tree of the model, so it is possible to continue training after loading the file. This, however, is at the cost of your memory when running the model, as it would be store in your ram on the fly. If all you need is the hidden weight, it can be extract from the model seperatly. The following code demonstrate how to save, load the model and extract the word vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "model = Word2Vec.load('item2vec_20180327')\n",
    "word_vectors = model.wv\n",
    "# del model # uncomment this line will delete the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's Make Some Recommendations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is trained, we can use built-in methods in Gensim to do recommendations! I have written some utility wrappers to enhance the ease-of-use. But what we are really utilizing is the Gensim model.wv.most_similar_word() method. The utilities would take a user’s input, from which infer the most likely movie names based on a search on IMDB, translate them into the movieIds and feed them into the Gensim method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def refine_search(search_term):\n",
    "    \"\"\"\n",
    "    Refine the movie name to be recognized by the recommender\n",
    "    Args:\n",
    "        search_term (string): Search Term\n",
    "\n",
    "    Returns:\n",
    "        refined_term (string): a name that can be search in the dataset\n",
    "    \"\"\"\n",
    "    target_url = \"http://www.imdb.com/find?ref_=nv_sr_fn&q=\"+\"+\".join(search_term.split())+\"&s=tt\"\n",
    "    html = requests.get(target_url).content\n",
    "    parsed_html = BeautifulSoup(html, 'html.parser')\n",
    "    for tag in parsed_html.find_all('td', class_=\"result_text\"):\n",
    "        search_result = re.findall('fn_tt_tt_1\">(.*)</a>(.*)</td>', str(tag))\n",
    "        if search_result:\n",
    "            if search_result[0][0].split()[0]==\"The\":\n",
    "                str_frac = \" \".join(search_result[0][0].split()[1:])+\", \"+search_result[0][0].split()[0]\n",
    "                refined_name = str_frac+\" \"+search_result[0][1].strip()\n",
    "            else:\n",
    "                refined_name = search_result[0][0]+\" \"+search_result[0][1].strip()\n",
    "    return refined_name\n",
    "\n",
    "def produce_list_of_movieId(list_of_movieName, useRefineSearch=False):\n",
    "    \"\"\"\n",
    "    Turn a list of movie name into a list of movie ids. The movie names has to be exactly the same as they are in the dataset.\n",
    "    Ambiguous movie names can be supplied if useRefineSearch is set to True\n",
    "    \n",
    "    Args:\n",
    "        list_of_movieName (List): A list of movie names.\n",
    "        useRefineSearch (boolean): Ambiguous movie names can be supplied if useRefineSearch is set to True\n",
    "\n",
    "    Returns:\n",
    "        list_of_movie_id (List of strings): A list of movie ids.\n",
    "    \"\"\"\n",
    "    list_of_movie_id = []\n",
    "    for movieName in list_of_movieName:\n",
    "        if useRefineSearch:\n",
    "            movieName = refine_search(movieName)\n",
    "            print(\"Refined Name: \"+movieName)\n",
    "        if movieName in name_to_movieId.keys():\n",
    "            list_of_movie_id.append(str(name_to_movieId[movieName]))\n",
    "    return list_of_movie_id\n",
    "\n",
    "def recommender(positive_list=None, negative_list=None, useRefineSearch=False, topn=20):\n",
    "    recommend_movie_ls = []\n",
    "    if positive_list:\n",
    "        positive_list = produce_list_of_movieId(positive_list, useRefineSearch)\n",
    "    if negative_list:\n",
    "        negative_list = produce_list_of_movieId(negative_list, useRefineSearch)\n",
    "    for movieId, prob in model.wv.most_similar_cosmul(positive=positive_list, negative=negative_list, topn=topn):\n",
    "        recommend_movie_ls.append(movieId)\n",
    "    return recommend_movie_ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code shows the top 5 recommendations to a user given that he/she likes the Disney movie “Up (2009)”:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refined Name: Up (2009)\n",
      "Recommendation Result based on \"Up (2009)\":\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11614</th>\n",
       "      <td>50872</td>\n",
       "      <td>Ratatouille (2007)</td>\n",
       "      <td>Animation|Children|Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12746</th>\n",
       "      <td>60069</td>\n",
       "      <td>WALL·E (2008)</td>\n",
       "      <td>Adventure|Animation|Children|Romance|Sci-Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14592</th>\n",
       "      <td>72998</td>\n",
       "      <td>Avatar (2009)</td>\n",
       "      <td>Action|Adventure|Sci-Fi|IMAX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15031</th>\n",
       "      <td>76093</td>\n",
       "      <td>How to Train Your Dragon (2010)</td>\n",
       "      <td>Adventure|Animation|Children|Fantasy|IMAX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15401</th>\n",
       "      <td>78499</td>\n",
       "      <td>Toy Story 3 (2010)</td>\n",
       "      <td>Adventure|Animation|Children|Comedy|Fantasy|IMAX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       movieId                            title  \\\n",
       "11614    50872               Ratatouille (2007)   \n",
       "12746    60069                    WALL·E (2008)   \n",
       "14592    72998                    Avatar (2009)   \n",
       "15031    76093  How to Train Your Dragon (2010)   \n",
       "15401    78499               Toy Story 3 (2010)   \n",
       "\n",
       "                                                 genres  \n",
       "11614                          Animation|Children|Drama  \n",
       "12746       Adventure|Animation|Children|Romance|Sci-Fi  \n",
       "14592                      Action|Adventure|Sci-Fi|IMAX  \n",
       "15031         Adventure|Animation|Children|Fantasy|IMAX  \n",
       "15401  Adventure|Animation|Children|Comedy|Fantasy|IMAX  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ls = recommender(positive_list=[\"UP\"], useRefineSearch=True, topn=5)\n",
    "print('Recommendation Result based on \"Up (2009)\":')\n",
    "display(df_movies[df_movies['movieId'].isin(ls)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1kxES3TgKLQpWo260WB_O5HHu-wY99gC2\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s try another one. My friend and I both like the science-fiction classic “The Matrix (1999)”. But when it comes to Quentin Tarantino's iconic \"Django Unchained (2012)\", we've got very different opinions. While I enjoy the ironic mix of absurdity and humor, my friend disgust the blood and violence. What would the model recommend based on our tastes? Supply those data into our model and ta-da ~ “Men in Black (1997)” and \"Ghostbusters (a.k.a Ghost Busters) are on my friend's recommendation list. And I've got \"Inglourious Basterds (2009)\" (of course!), \"Inception (2010)\", and \"The Dark Knight Rises (2006)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refined Name: Matrix, The (1999)\n",
      "Refined Name: Django Unchained (2012)\n",
      "Recommendation Result based on \"The Matrix (1999)\" minus \"Django Unchained (2012)\":\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1172</th>\n",
       "      <td>1197</td>\n",
       "      <td>Princess Bride, The (1987)</td>\n",
       "      <td>Action|Adventure|Comedy|Fantasy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1528</th>\n",
       "      <td>1580</td>\n",
       "      <td>Men in Black (a.k.a. MIB) (1997)</td>\n",
       "      <td>Action|Comedy|Sci-Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1557</th>\n",
       "      <td>1610</td>\n",
       "      <td>Hunt for Red October, The (1990)</td>\n",
       "      <td>Action|Adventure|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1707</th>\n",
       "      <td>1777</td>\n",
       "      <td>Wedding Singer, The (1998)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1839</th>\n",
       "      <td>1923</td>\n",
       "      <td>There's Something About Mary (1998)</td>\n",
       "      <td>Comedy|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2630</th>\n",
       "      <td>2716</td>\n",
       "      <td>Ghostbusters (a.k.a. Ghost Busters) (1984)</td>\n",
       "      <td>Action|Comedy|Sci-Fi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>2918</td>\n",
       "      <td>Ferris Bueller's Day Off (1986)</td>\n",
       "      <td>Comedy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      movieId                                       title  \\\n",
       "1172     1197                  Princess Bride, The (1987)   \n",
       "1528     1580            Men in Black (a.k.a. MIB) (1997)   \n",
       "1557     1610            Hunt for Red October, The (1990)   \n",
       "1707     1777                  Wedding Singer, The (1998)   \n",
       "1839     1923         There's Something About Mary (1998)   \n",
       "2630     2716  Ghostbusters (a.k.a. Ghost Busters) (1984)   \n",
       "2832     2918             Ferris Bueller's Day Off (1986)   \n",
       "\n",
       "                                       genres  \n",
       "1172  Action|Adventure|Comedy|Fantasy|Romance  \n",
       "1528                     Action|Comedy|Sci-Fi  \n",
       "1557                Action|Adventure|Thriller  \n",
       "1707                           Comedy|Romance  \n",
       "1839                           Comedy|Romance  \n",
       "2630                     Action|Comedy|Sci-Fi  \n",
       "2832                                   Comedy  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ls = recommender(positive_list=[\"The Matrix\"], negative_list=[\"Django Unchained\"], useRefineSearch=True, topn=7)\n",
    "print('Recommendation Result based on \"The Matrix (1999)\" minus \"Django Unchained (2012)\":')\n",
    "display(df_movies[df_movies['movieId'].isin(ls)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=111Y0BQ4nAhxzrKRGgqu60E9aGrHR_EDh\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refined Name: Matrix, The (1999)\n",
      "Refined Name: Django Unchained (2012)\n",
      "Recommendation Result based on \"The Matrix (1999)\" + \"\"Django Unchained (2012)\":\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movieId</th>\n",
       "      <th>title</th>\n",
       "      <th>genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11401</th>\n",
       "      <td>48780</td>\n",
       "      <td>Prestige, The (2006)</td>\n",
       "      <td>Drama|Mystery|Sci-Fi|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12525</th>\n",
       "      <td>58559</td>\n",
       "      <td>Dark Knight, The (2008)</td>\n",
       "      <td>Action|Crime|Drama|IMAX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13102</th>\n",
       "      <td>63082</td>\n",
       "      <td>Slumdog Millionaire (2008)</td>\n",
       "      <td>Crime|Drama|Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13647</th>\n",
       "      <td>68157</td>\n",
       "      <td>Inglourious Basterds (2009)</td>\n",
       "      <td>Action|Drama|War</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14009</th>\n",
       "      <td>70286</td>\n",
       "      <td>District 9 (2009)</td>\n",
       "      <td>Mystery|Sci-Fi|Thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15534</th>\n",
       "      <td>79132</td>\n",
       "      <td>Inception (2010)</td>\n",
       "      <td>Action|Crime|Drama|Mystery|Sci-Fi|Thriller|IMAX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18312</th>\n",
       "      <td>91529</td>\n",
       "      <td>Dark Knight Rises, The (2012)</td>\n",
       "      <td>Action|Adventure|Crime|IMAX</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       movieId                          title  \\\n",
       "11401    48780           Prestige, The (2006)   \n",
       "12525    58559        Dark Knight, The (2008)   \n",
       "13102    63082     Slumdog Millionaire (2008)   \n",
       "13647    68157    Inglourious Basterds (2009)   \n",
       "14009    70286              District 9 (2009)   \n",
       "15534    79132               Inception (2010)   \n",
       "18312    91529  Dark Knight Rises, The (2012)   \n",
       "\n",
       "                                                genres  \n",
       "11401                    Drama|Mystery|Sci-Fi|Thriller  \n",
       "12525                          Action|Crime|Drama|IMAX  \n",
       "13102                              Crime|Drama|Romance  \n",
       "13647                                 Action|Drama|War  \n",
       "14009                          Mystery|Sci-Fi|Thriller  \n",
       "15534  Action|Crime|Drama|Mystery|Sci-Fi|Thriller|IMAX  \n",
       "18312                      Action|Adventure|Crime|IMAX  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ls = recommender(positive_list=[\"The Matrix\", \"Django Unchained\"], useRefineSearch=True, topn=7)\n",
    "print('Recommendation Result based on \"The Matrix (1999)\" + \"\"Django Unchained (2012)\":')\n",
    "display(df_movies[df_movies['movieId'].isin(ls)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?id=1i7qKVIL-ZjK76JnC_7IPiVIUVLVkN_2w\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would our model perform in general when facing new data? Remember that we trained several models with different parameters. Which one is better?\n",
    "\n",
    "To figure out the answer, an evaluation metric has to be defined. Our model produces a fixed number of recommendations on each run.The performance of this task among different models can be evaluated by the precision, recall, and F1-Score at K (K being the number of recommendations we made for each input).\n",
    "\n",
    "In the context of our movie recommender system, precision is the ratio between “the number of successful recommendations” and “the number of all the recommendations made”. And recall is the ratio between “the number of successful recommendations” and “the number of all the movies that the user truly likes\". F1-Score is the harmonic average of precision and recall.\n",
    "\n",
    "Note that the more movies a user liked, the more likely the precision would be high and the recall would be low. On the other hand, given a fixed number of \"Liked\" movies, the more recommendations our model made (a larger K), the more likely the precision would be low and the recall would be high. Those are the things we need to keep in mind when interpreting precision and recall at K on a single model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code extract the \"LIKED\" movies from the users, and evlauate the precision, recall and F1-score. Scores both models I've trained are provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def user_liked_movies_builder(model, df, for_prediction=False):\n",
    "    df['liked'] = np.where(df['rating']>=4, 1, 0)\n",
    "    df['movieId'] = df['movieId'].astype('str')\n",
    "    df_liked = df[df['liked']==1]\n",
    "    if for_prediction:\n",
    "        df_liked = df[df['movieId'].isin(model.wv.vocab.keys())]\n",
    "        \n",
    "    user_liked_movies = df_liked.groupby('userId').agg({'movieId': lambda x: x.tolist()})['movieId'].to_dict()\n",
    "    \n",
    "    return user_liked_movies\n",
    "\n",
    "def scores_at_m (model, user_liked_movies_test, user_liked_movies_training, topn=10):\n",
    "    sum_liked = 0\n",
    "    sum_correct = 0\n",
    "    sum_total = 0\n",
    "    common_users = set(user_liked_movies_test.keys()).intersection(set(user_liked_movies_training.keys()))\n",
    "\n",
    "    for userid in common_users:\n",
    "        current_test_set = set(user_liked_movies_test[userid])\n",
    "        pred = [pred_result[0] for pred_result in model.wv.most_similar_cosmul(positive = user_liked_movies_training[userid], topn=topn)]\n",
    "        sum_correct += len(set(pred).intersection(current_test_set))\n",
    "        sum_liked += len(current_test_set)\n",
    "    precision_at_m = sum_correct/(topn*len(common_users))\n",
    "    recall_at_m = sum_correct/sum_liked\n",
    "    f1 = 2/((1/precision_at_m)+(1/recall_at_m))\n",
    "    return [precision_at_m, recall_at_m, f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "user_liked_movies_train = user_liked_movies_builder(model, df_ratings_train, for_prediction=True)\n",
    "user_liked_movies_test = user_liked_movies_builder(model, df_ratings_test)\n",
    "\n",
    "model = Word2Vec.load('item2vec_20180327')\n",
    "model_score_sg1 = scores_at_m(model, user_liked_movies_test, user_liked_movies_train)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respectively, the [precision, recall, F-1 score] at 10 for our model are:\n",
      "[0.15050706688037302, 0.06892699815926419, 0.09455232322242973]\n"
     ]
    }
   ],
   "source": [
    "print(\"Respectively, the [precision, recall, F-1 score] at 10 for our model are:\")\n",
    "print(model_score_sg1)  ## congrats!! haha bylu! yating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None\n",
    "model = Word2Vec.load('item2vec_word2vecSg_20180328')\n",
    "\n",
    "user_liked_movies_train = user_liked_movies_builder(model, df_ratings_train, for_prediction=True)\n",
    "user_liked_movies_test = user_liked_movies_builder(model, df_ratings_test)\n",
    "\n",
    "model_score_sg2 = scores_at_m(model, user_liked_movies_test, user_liked_movies_train)\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respectively, the [precision, recall, F-1 score] at 10 for the model are:\n",
      "[0.12680387585603964, 0.058071761671255995, 0.07966138271319025]\n"
     ]
    }
   ],
   "source": [
    "print(\"Respectively, the [precision, recall, F-1 score] at 10 for the model are:\")\n",
    "print(model_score_sg2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! We've got our own recommender system with Item2vec. To explore more, do read the papers and projects in the references! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='mccormick'></a>\n",
    "##### [1] McCormick, C. (2016, April 19). Word2Vec Tutorial - The Skip-Gram Model. Retrieved from http://www.mccormickml.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [2] Oren Barkan and Noam Koenigstein. Item2vec: Neural item embedding for collaborative filtering. 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [3] Vitali Kuzmin. Item2Vec-based Approach to a Recommender System. 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [4] Doosan Jung. Item2vec project. https://github.com/DoosanJung/I2V_project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### [5] Makbule Gulcin Ozsoy. From Word Embeddings to Item Recommendation. 2016"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
